{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9711e9-7b0b-471c-85e3-079922b17c96",
   "metadata": {},
   "source": [
    "# Essential Guide to Python Pandas <a class=\"anchor\" id=\"title\"></a>\n",
    "\n",
    "### A Crash Course with Reusable Code Template in Jupyter Notebook\n",
    "\n",
    "The Pandas library has emerged as one of the most important data wrangling and processing tools for Python developers and data professionals. It allows users to quickly apply data wrangling tasks such as handling missing data, removing duplicate records, merging multiple datasets, and so on. Therefore, the Pandas library has become a must-have tool in the data science toolkit.\n",
    "\n",
    "In this series of articles, we provide a crash course to get you started using the Pandas library. The course is designed to be a practical guide with real-life examples of the most common data manipulation tasks.\n",
    "\n",
    "### Who is this Course for\n",
    "\n",
    "This course is for aspiring data professionals and Python developers who want to learn how to process data in Pandas. We assume you already have a minimum working knowledge about Python programming language and are comfortable running data science documents using Jupyter notebook. \n",
    "\n",
    "To follow the examples in this course, you can copy and paste the code snippets into your Jupyter notebook environment.\n",
    "\n",
    "### What Makes Pandas Special\n",
    "\n",
    "Pandas is an open-source, free (under a [BSD license](https://en.wikipedia.org/wiki/BSD_licenses)) Python library originally written by [Wes McKinney](https://en.wikipedia.org/wiki/Wes_McKinney). It is a high-level data structure and manipulation tool designed to make data analysis and wrangling fast and easy. The library offers a variety of functions and methods to transfer different data sources into a tabular format. In this format, each record is housed on one row and each column contains a unique data type. \n",
    "\n",
    "Python Developers and Data Professionals can upload data from a variety of data sources such as SQL Databases, API JSON format, CSV files as well as native Python data structures like lists and dictionaries. This flexibility makes Pandas suitable for a wide range of applications. Among them are machine learning modeling, data visualization, and time series forecasting. The versatile structure also makes it easy to integrate pandas with other libraries such as scikit-learn. \n",
    "\n",
    "Early [releases of Pandas](https://pandas.pydata.org/docs/whatsnew/index.html) DataFrame dated back to 2011 with Pandas version 1.0 released in January 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54c162-a6fa-4eb2-9560-c12eac6854f4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [ How to Import Pandas Library](#section_1)\n",
    "2. [ Anatomy of Pandas Data Structures](#section_2)\n",
    "3. [ Get Data into and from Pandas](#section_3)\n",
    "    * Python Native Data Structures\n",
    "    * Tabular Data Files\n",
    "    * API Query and JSON Format\n",
    "    * Web Pages Data\n",
    "4. [Describe Information in DataFrames](#section_4)\n",
    "5. [Understand Data Types](#section_5)\n",
    "6. [Data Cleaning in Pandas](#section_6)\n",
    "    * Split & Merge Columns\n",
    "    * Change Columns DataType\n",
    "    * Rename Columns\n",
    "    * Drop Rows and Columns\n",
    "    * Manipulate text content\n",
    "7. [Pandas Merging & Joining Data](#section_7)\n",
    "8. [Data Accessing & Aggregation](#section_8)\n",
    "9. [Pandas Data Visualization](#section_9)\n",
    "10. [Pandas Analysis Project](#section_10)\n",
    "    * Collect Data From Multiple Sources\n",
    "    * Clean Data\n",
    "    * Join DataFrames\n",
    "    * Perform Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014f463-7d9b-4644-b2d7-b303aef73201",
   "metadata": {},
   "source": [
    "### 1. How to Import Pandas Library <a class=\"anchor\" id=\"section_1\"></a>\n",
    "The easiest way to start using Pandas library is to get the Python [Anaconda](https://www.anaconda.com/products/individual#Downloads) distribution, a cross-platform distribution for data analysis and scientific computing. The distribution has more than 250 of the most commonly used data science packages and tools such as Pandas, Scikit Learn, Jupyter, and so on. To start using Pandas in your analysis environment, you need to simply run the import Pandas command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76fadd-c2d0-4195-ad78-b0692a594cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acbf071-b9ce-4248-a85f-422faf295921",
   "metadata": {},
   "source": [
    "To check your current version of Pandas library, you can run the __version__ command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e51eb4-3b22-426b-8ce7-927e3f8db257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Pandas version\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55f22d-28a4-44ec-89ac-0a202cea7e2a",
   "metadata": {},
   "source": [
    "Great, you are now ready to start learning Pandas library!\n",
    "\n",
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528cdc6-1281-41c0-9a8d-8ac8d66c0f50",
   "metadata": {},
   "source": [
    "### 2. Anatomy of Pandas Data Structures <a class=\"anchor\" id=\"section_2\"></a>\n",
    "\n",
    "\n",
    "The two main Pandas data structure objects are **DataFrames** and **Series**. Pandas DataFrame object is a two-dimensional labeled structure that can hold data in rows and columns, similar to a spreadsheet file or relational database table. Each DataFrame column (also called Pandas Series Object) is a one-dimensional labeled structure with a descriptive name and unique data type that applies to all values in that column. ***In other words, you can think of a DataFrame as a collection of Series.***\n",
    "\n",
    "Both DataFrame and Series objects have index keys that can be used to reference corresponding values. Index keys are created automatically and can be manipulated by the user to assign specific values as DataFrame or Series index. \n",
    "\n",
    "In the example below, we see a Pandas DataFrame object about countries. The DataFrame consists of four different Series objects (country_name, capital_city, population, area_km2) with index values representing each countryâ€™s ISO code. Later on, you will learn how you can use the DataFrame index to select specific data values. \n",
    "\n",
    "The image below demonstrates the structure of Pandas DataFrame and Series objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383755f-1538-4871-8522-b0a4d85822bd",
   "metadata": {},
   "source": [
    "<img src='Images/DataFrame Stracture.png' class=\"center\"/>\n",
    "\n",
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4159a3-9b62-4e25-b1c3-e9e642a77106",
   "metadata": {},
   "source": [
    "### 3. Getting Data into and from Pandas <a class=\"anchor\" id=\"section_3\"></a>\n",
    "\n",
    "Pandas library is designed to access data from a wide variety of sources and formats. Some popular data sources include tabular files, database tables, third-party APIs and even using Python native data structures. This flexibility is what makes Pandas library useful for many user groups such as developers and data professionals.\n",
    "\n",
    "To upload data into a Pandas DataFrame, you can utilize a set of reader functions such as [pandas.read_csv()]() to get the data into DataFrame objects. The library also has a set of writer functions such as [pandas.DataFrame.to_csv()]() to allow users to export data frames into external dataset files. \n",
    "\n",
    "In each function, you can use a set of parameters to pass specific information about your dataset. For instance, in the [pandas.read_csv()]() and [pandas.read_table()]() functions, you can use the sep parameter to identify the delimiter that separates your data values. \n",
    "\n",
    "The figure below shows a list of available readers and writers functions. \n",
    "\n",
    "<img src='Images/Pandas_io_readwrite.svg' class=\"center\"/>\n",
    "\n",
    "Source: [Pandas IO Tools]()\n",
    "\n",
    "In the following sections, we will learn about some most commonly used methods to get data into and from a Pandas DataFrame.\n",
    "\n",
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba06386-ff7e-408b-b010-543c533ebbaf",
   "metadata": {},
   "source": [
    "#### 3.1 Python Native Data Structures\n",
    "The Python programming language has a variety of built-in data structures such as lists, tuples, dictionaries, strings, and sets. These data structures are ideal for storing data during program execution, however, they can not be efficiently used to perform analytical tasks such as exploratory analysis and data visualization. Pandas library can transfer Python data structures into DataFrame objects to allow users to easily perform data manipulation and analytics. \n",
    "\n",
    "For example, imagine we have a Python dictionary to save country information such as below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58eca4f-0718-4a96-9941-e9a2d5096479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python dictionary\n",
    "{'country_name':'New Zealand',\n",
    " 'capital_city':'Wellington',\n",
    " 'country_code':'NZ',\n",
    " 'population':4783063,\n",
    " 'area_km2':270467}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff252c-5d04-4b0b-b9f0-4e3fa3424141",
   "metadata": {},
   "source": [
    "The dictionary key represents the attribute label or title while the dictionary value represents the corresponding information. Pandas library can convert a list of similar dictionaries into a DataFrame object as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67b5a7-3e14-4743-8329-7330782c0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries\n",
    "list_of_countries = [\n",
    "{'country_name':'China','capital_city':'Beijing','population':1433783686,'area_km2':9596961},\n",
    "{'country_name':'New Zealand','capital_city':'Wellington','population':4783063,'area_km2':270467},\n",
    "{'country_name':'South Africa','capital_city':'Pretoria','population':58558270,'area_km2':1221037},\n",
    "{'country_name':'United Kingdom','capital_city':'London','population':67530172,'area_km2':242495},\n",
    "{'country_name':'United States','capital_city':'Washington DC','population':329064917,'area_km2':9525067}]\n",
    "\n",
    "# Create a Pandas DataFrame from a list of dictionaries\n",
    "countries = pd.DataFrame(list_of_countries, index = ['CN','NZ','ZA','GB','US'])\n",
    "\n",
    "# Display the DataFrame\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f688203-9606-4303-8832-0c67de323960",
   "metadata": {},
   "source": [
    "In the code above, we notice the variable list_of_countries is defined as a Python list with each element representing a Python dictionary of country information. We import the Pandas library and then we use the built-in DataFrame function to transfer the list of countries into a Pandas Dataframe called countries. The pd.DataFrame is a built-in function to construct DataFrame objects from scratch or from native Python data structures. \n",
    "\n",
    "Notice how we used the index parameter to pass a list of country codes as our DataFrame index values. We can then examine the new DataFrame object using the built-in head() function to return the top rows. We will learn later about the different ways to examine any DataFrame content. \n",
    "\n",
    "We notice how the dictionary keys were assigned as the DataFrame column names while dictionary values are assigned as the cells. A numerical index with values between 0 to 4 was automatically assigned to the DataFrame object. The user can choose to pass specific index values by using the index parameter as shown in the example above.\n",
    "\n",
    "Another approach is to use a Python dictionary where keys represent column names and values represent Python lists. You can then make use of Pandas [from_dict()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_dict.html) function to transfer the dictionary into a DataFrame object as shown in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9bdb4-a248-466c-92fd-f4844598dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary where keys represent column names and values represent Python lists.\n",
    "dictionary_of_countries = {'country_name': ['China', 'New Zealand', 'South Africa', 'United Kingdom', 'United States'],\n",
    "                           'country_code': ['CN', 'NZ', 'ZA', 'GB', 'US'],\n",
    "                           'capital_city': ['Beijing', 'Wellington', 'Pretoria', 'London', 'Washington DC'],\n",
    "                           'population': [1433783686, 4783063, 58558270, 67530172, 329064917],\n",
    "                           'area_km2': [9596961, 270467, 1221037, 242495, 9525067]}\n",
    "\n",
    "# Convert a dictionary into a DataFrame using from_dict() function\n",
    "countries = pd.DataFrame.from_dict(dictionary_of_countries)\n",
    "\n",
    "# Display the DataFrame\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7fad10-96a6-4f5a-b603-bcf65cfc0a12",
   "metadata": {},
   "source": [
    "The above examples demonstrated the flexibility of transforming data stored in Python native data structures into Pandas DataFrame objects. \n",
    "\n",
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e3056-e6d4-48ea-a38d-259e0b25634c",
   "metadata": {},
   "source": [
    "#### 3.2 Tabular Data Files\n",
    "Tabular data is usually structured into rows and columns and presented in various file formats including CSV, tab-delimited files, fixed-width formats, and spreadsheets. Tabular files can be accessed from the local computer or online. \n",
    "\n",
    "In this section, we will learn about how to access data from CSV files, Excel Sheet files, and SQL tables. First, we access a CSV file for alcohol consumption by country accessed from the fivethirtyeight [GitHub]() Repository. To do that, we use the [read_csv()]() function and pass the file online location on GitHub. If the CSV file is stored on the local machine, we need to pass the file path. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455b142-afd2-430e-ace4-d958e0cb085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame object using read_csv() function\n",
    "alcohol_data = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "alcohol_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9737bc4-44d6-43bd-9fe2-d35e3eebcea8",
   "metadata": {},
   "source": [
    "Another commonly used tabular data format is spreadsheets. Pandas library provides the [read_excel()]() built-in function to access Microsoft Excel spreadsheet files as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bffbd8-8c2e-4322-ba25-b69df3114bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame object using read_excel() function\n",
    "pd.read_excel(\"path_to_file/myFile.xls\", sheet_name=\"Sheet1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c207d-a6b6-4677-a054-22004b637b06",
   "metadata": {},
   "source": [
    "Notice how the [read_excel()]() example makes use of the sheet_name parameter to tell the system which sheet name contains the needed dataset. For a complete list of all parameters for each built-in function, check the Pandas official documentation by clicking the function name. \n",
    "\n",
    "Another common scenario is to query relational database tables using SQL language. Obviously, you would need to provide the necessary credentials and metadata to establish a connection with the database server. You can then apply [read_sql()]() function to pass the SQL query and load the result into a Pandas DataFrame object. \n",
    "\n",
    "To simulate this scenario, the following code will create a local database using the Python SQLite engine. We will then use Pandas to access the data using SQL queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49290c66-e575-4a3e-809d-83be22585c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SQLite library\n",
    "import sqlite3\n",
    "\n",
    "# Assign the database name\n",
    "db_path = r'local_db_example.db'\n",
    "\n",
    "# Create the database file\n",
    "conn = sqlite3.connect(db_path) \n",
    "\n",
    "# Establish a connection with the database file\n",
    "c = conn.cursor() \n",
    "\n",
    "# Create a database table\n",
    "c.execute(\"\"\"CREATE TABLE mytable\n",
    "         (id, name, position)\"\"\")\n",
    "\n",
    "\n",
    "# Add some data\n",
    "c.execute(\"\"\"INSERT INTO mytable (id, name, position)\n",
    "          values(1, 'James', 'Data Scientist')\"\"\")\n",
    "\n",
    "c.execute(\"\"\"INSERT INTO mytable (id, name, position)\n",
    "          values(2, 'Mary', 'Software Developer')\"\"\")\n",
    "\n",
    "c.execute(\"\"\"INSERT INTO mytable (id, name, position)\n",
    "          values(3, 'Max', 'Data Engineer')\"\"\")\n",
    "\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91ad22-c07c-42f5-b601-5e719a99a439",
   "metadata": {},
   "source": [
    "The relational database name `local_db_example.db` should appear as an external file in the same location with your notebook. The database file already includes dummy data describing employee details. The following code queries the data into a Pandas DataFrame object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aca0dd-3de9-4266-84be-e3fc7050dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the database name\n",
    "database = \"local_db_example.db\"\n",
    "\n",
    "# Establish a connection with the database file\n",
    "conn = sqlite3.connect(database)\n",
    "\n",
    "# Use Pandas function to pass SQL query and create a DataFrame object\n",
    "people = pd.read_sql(\"select * from mytable\", con=conn)\n",
    "\n",
    "# Print the generated DataFrame\n",
    "print(people)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fd64e-3f16-481b-9c6c-d8b8693d2199",
   "metadata": {},
   "source": [
    "In the above example, we created a local database file and used the Pandas library to query the data using SQL, and passed the results into a Pandas DataFrame object. In more practical examples, you may need to query data from relational databases that are stored on remote servers or in the cloud.\n",
    "\n",
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de5833-cf6c-4d48-85bf-5a448d6e9ebf",
   "metadata": {},
   "source": [
    "#### 3.3 API Query and JSON Format\n",
    "When working on daily tasks, data professionals often need to access data from third-party APIs. This approach is common when the data is continually updated like weather forecasting or when you need to select a small subset of data. API response data usually comes in JSON format which you can think of as a collection of Python data structures like dictionaries and lists represented as text. \n",
    "\n",
    "For example, we will use API data from [open-notify]() to get information about the International Space Station ISS. The API gives information about the space station location, altitude, and crow members. The following code will make a query about current crew members onboard ISS. In this example, we will make use of the Python request library to establish a connection with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb84da-121b-4b2d-9bc2-f11271779e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requests library to handle API connection\n",
    "import requests\n",
    "\n",
    "# Import and initialize Data pretty printer library\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Pass the API query using requests library\n",
    "response = requests.get(\"http://api.open-notify.org/astros.json\")\n",
    "# print(response.status_code)\n",
    "\n",
    "# Convert response data into JSON format\n",
    "response_data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9cb03-9702-4fc0-8452-d88986bf29a1",
   "metadata": {},
   "source": [
    "Once we have the API response data, we notice the response includes a list of dictionaries about the astronauts currently aboard the ISS. We can convert this part of the response into a Pandas DataFrame object as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bc9a1-40a3-4844-9f29-79630b01be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the response data\n",
    "pp.pprint(response_data)\n",
    "\n",
    "# Create a DataFrame of astronauts currently aboard the ISS\n",
    "astronauts = pd.DataFrame(response_data['people'])\n",
    "\n",
    "# Display the DataFrame\n",
    "astronauts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ab7ba-3d95-4371-9077-6dd3e3be9c2c",
   "metadata": {},
   "source": [
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30055d8e-403c-4196-902d-e70a50eb8a34",
   "metadata": {},
   "source": [
    "#### 3.4 Web Pages Data\n",
    "Pandas library offers a built-in function to allow users to parse HTML tables from web pages into a list of Pandas DataFrames. This functionality provides users with a fast way to access data tables embedded in web pagesâ€™ html code. To demonstrate the process, we will use the Pandas function [read_html()]() to parse the [list of countries by population table]() from Wikipedia into a DataFrame object as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36305c43-b1d0-4de8-8fbc-4c772a005983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HTML table tags from the web page\n",
    "web_data = pd.read_html('https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)')\n",
    "\n",
    "# Display the data type\n",
    "type(web_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07708918-d5b0-4e88-9287-64151029cc43",
   "metadata": {},
   "source": [
    "When examining the type of web_data variable, we notice the [read_html()]() function has returned a list of five elements representing the table tags detected in the webpage HTML code. Each table tag was automatically converted into a Pandas DataFrame object. \n",
    "\n",
    "However, not all tables are useful as they may contain unwanted HTML data. Therefore, we must carefully examine the returned list and identify the useful DataFrame objects. \n",
    "\n",
    "In this example, we notice the first item `web_data[0]` contains the needed countries table. Therefore, we can assign the fourth item to a new variable for easier use as shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee235c7-08ee-4a81-ac03-2b37cdba88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first item in the list\n",
    "web_countries_table = web_data[0]\n",
    "\n",
    "# Display the DataFrame\n",
    "web_countries_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2faf5-a9f8-4c8a-af5c-0ca277edd86f",
   "metadata": {},
   "source": [
    "By examining the first few rows of our DataFrame, we notice the parsed table includes some unwanted strings such as brackets `[ ]` and parentheses `( )` in country names and column titles. \n",
    "\n",
    "Also, we notice the column Change includes both real numbers and mathematical symbols which would transfer the column into a text datatype (we will learn more about Pandas data types soon). These issues are normal for data accessed from web pages and may not be readily available for data analysis. \n",
    "\n",
    "Luckily, Pandas library includes some other tools and functions that would help the user to clean up the data for analysis. The use of the [read_html()]() function would save time from web data using more traditional web scraping libraries such as Beautiful Soup. In later examples, we will learn more about how to convert similar tables from web pages into DataFrames ready for data analysis.\n",
    "\n",
    "In addition to the above scenarios for creating DataFrame objects using reader functions, the Pandas library also provides a set of writer functions to save DataFrame objects as external datasets.\n",
    "\n",
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5a66c-ded1-4e01-b1e1-06c24c822ca5",
   "metadata": {},
   "source": [
    "### 4.0 Describe Information in DataFrames  <a class=\"anchor\" id=\"section_4\"></a>\n",
    "\n",
    "In the previous section, we learned how to use Pandas reader and writer functions to create DataFrame objects from different data sources. Once the data is uploaded, users can make use of several built-in attributes designed to examine the states of the DataFrames. These attributes can provide users with information about the DataFrame size, data types, missing values, in addition to basic summary statistics. \n",
    "\n",
    "This information is important for users to identify what changes they would need to make to prepare the DataFrame object for further analysis. \n",
    "\n",
    "To demonstrate commands, we will use Pandas reader functions to import some publicly available datasets from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa0e44-9de2-4f7d-a585-2398912b006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset from GitHub repository\n",
    "alcohol_data = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv')\n",
    "\n",
    "# Display DataFrame\n",
    "alcohol_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b966e6-585f-44bc-8342-c9019497d9ab",
   "metadata": {},
   "source": [
    "One of the first questions to answer when working with a new dataset is to know the size of the data, i.e., how many rows and columns we have. \n",
    "\n",
    "To answer this question, we can use the shape attribute which returns a Python tuple representing the dimensionality of DataFrame objects. The first value represents the number of records while the second value counts the number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda1de3-60a8-43d4-85c3-44226214466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimension of alcohol_data DataFrame\n",
    "Alcohol_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac26fa8-973c-4a5b-80fc-fbcc3d638e0a",
   "metadata": {},
   "source": [
    "Another commonly used attribute is size, which can be used to identify how many elements we have in a given DataFrame or Series object (including missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538f6ee-b858-4534-bca5-d1929d787397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many elements in alcohol_data DataFrame\n",
    "Alcohol_data.size\n",
    "\n",
    "# How many elements in alcohol_data[`country`] Series\n",
    "Alcohol_data[`country`].size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65795fa-e1c8-419e-9cc8-868b4c069f7f",
   "metadata": {},
   "source": [
    "Once we have learned about the size of our DataFrame and Series objects, we can learn more details using the `info()` attribute. It provides a useful summary of the DataFrame columns as shown in the  example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44081e8f-e9c5-4529-9ce1-a6cbba44bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of the DataFrame columns\n",
    "alcohol_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efca385-c68b-4a25-9e64-0715a8c2c767",
   "metadata": {},
   "source": [
    "The results first highlight the number of records in the DataFrame and the range of the numerical index value automatically assigned to this DataFrame. It shows the total number of columns (5 columns in our dataset). Next, it lists the column names with their respective data types and how many values of that column contain an empty or null value. \n",
    "\n",
    "In this dataset, it seems we donâ€™t have any missing values since the number of records is equal to the number of non-null counts. We notice the data types for the country column is Pandas objects which represent text values, while three servings columns (`beer_servings`, `spirit_servings`, and `wine_servings`) have the int64 data type which represents integer numbers, and total litres column assigned float64 data type which allows real numbers.\n",
    "\n",
    "At this stage, we have an idea about what changes we need to make in order to have the correct data types. For example, numerical data types such as int64 allow us to apply mathematical calculations on the values while object data type allows us to apply text formatting functions. In the next section about data cleaning, we will learn how to change data types. \n",
    "\n",
    "Finally, the function displays data about how many columns there are for each data type and the memory size of this DataFrame (the memory size info can be useful when working with a large DataFrame and you may wish to optimize the DataFrame size).\n",
    "\n",
    "The other exploratory attribute [describe()]() will return basic statistical analysis of the DataFrame numeric columns as shown in this example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86541d12-9f4f-4055-8e1d-e29c34b50d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical analysis of the DataFrame\n",
    "alcohol_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3082fcd-5d9a-44c9-9156-6b07519a7f98",
   "metadata": {},
   "source": [
    "From the example above, we notice the [describe()]() function was only applied to the numerical columns and the country name column was ignored. This is because descriptive statistics are based on numerical columns only to summarize the central tendency, dispersion, and shape of a datasetâ€™s distribution, excluding NaN values.\n",
    "\n",
    "In addition to the numerical statistical summary, you can also explore the features of text values in DataFrames. For this exercise, we will use the [country codes dataset]() from the [Open Data GitHub repository](). The data include many details about each country's international codes and geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a440ae4-bf94-4b68-ad01-25c7cd6fda0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset from GitHub repository\n",
    "countries_data = pd.read_csv('https://raw.githubusercontent.com/datasets/country-codes/master/data/country-codes.csv')\n",
    "\n",
    "# Display DataFrame head\n",
    "countries_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506b4a3-7abc-4caa-8263-786559728d55",
   "metadata": {},
   "source": [
    "For instance, the column Region Name appears to be a text column that holds the geographical region of each country. In order to find the number of individual region values we can apply functions like [unique()](), and [value_counts()]() on Pandas series values like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ddd02-416d-4ffd-8e4b-db31b602e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique individual region names\n",
    "countries_data['Region Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24e395-044d-42f4-9cfb-309b2ac08634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of individual region names\n",
    "countries_data['Region Name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d378bcc-8ad2-45e0-93c2-5d155547df82",
   "metadata": {},
   "source": [
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab591659-1314-4050-bc03-8a32b0cf219c",
   "metadata": {},
   "source": [
    "### 5.0 Understanding Data Types <a class=\"anchor\" id=\"section_5\"></a>\n",
    "\n",
    "It is important to assign the correct data type for each DataFrame column in order to avoid any problems for data analysis. Pandas will try to infer the correct data type for each column. For a list of Pandas data type mapping, please refer to this link. \n",
    "\n",
    "However, sometimes you need to change the data type manually. Selecting the correct data type will allow you to perform further analysis such as mathematical analysis on numeric columns and text formatting on object data types. The following table presents common Pandas data types:\n",
    "\n",
    "| Pandas DataType | Usage |\n",
    "| --- | --- |\n",
    "| object | Text or mixed numeric and non-numeric values |\n",
    "| int64 | Integer numbers |\n",
    "| float64 | Floating point numbers |\n",
    "| bool | True/False values |\n",
    "| datetime64 | Date and time values |\n",
    "| timedelta[ns] | Differences between two datetimes |\n",
    "| category | Finite list of text values |\n",
    "\n",
    "We will learn how to change the Pandas data types in the upcoming part of the handbook.\n",
    "\n",
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a4dfe-22ec-4d19-a23f-d7e4795ef873",
   "metadata": {},
   "source": [
    "### 6.0 Data Cleaning in Pandas <a class=\"anchor\" id=\"section_6\"></a>\n",
    "\n",
    "Previously, we learned how to use the different Pandas tools and functions to get external datasets into a DataFrame object. Many real-life datasets come with problems such as missing values, wrong datatype, and bad formatting. Data professionals usually need to spend lots of time correcting these issues before the dataset becomes ready for analysis. Luckily, Pandas library comes with a set of built-in functions to help users fix these issues. In this section, we will learn how to use Pandas to identify and correct some common data quality issues. \n",
    "\n",
    "To demonstrate the process, we will use a toy DataFrame about countries. Each country has different pieces of information such as name, population, size, and independence date as shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53e8e7-126e-47cd-ad60-53cf4513dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries\n",
    "list_of_countries = [\n",
    "{'Country Name':'China','ISO Code':'CN','Country Population':1433783686,'Country Area km2 (mi2)':'9,596,961 (3,705,407)','Independence Day':'1 October 1949'},\n",
    "{'Country Name':'New Zealand','ISO Code':'NZ','Country Population':4783063,'Country Area km2 (mi2)':'270,467 (104,428)','Independence Day':'26 September 1907'},\n",
    "{'Country Name':'South Africa','ISO Code':'ZA','Country Population':58558270,'Country Area km2 (mi2)':'1,221,037 (471,445)','Independence Day':'31 May 1910'},\n",
    "{'Country Name':'Australia','ISO Code':'AU','Country Population':25763300,'Country Area km2 (mi2)':'7,692,024 (2,969,907)', 'Independence Day':'1 January 1901'},\n",
    "{'Country Name':'United States','ISO Code':'US','Country Population':329064917,'Country Area km2 (mi2)':'9,525,067 (3,677,649)','Independence Day':'4 July 1776'},\n",
    "{'Country Name':'New Zealand','ISO Code':'NZ','Country Population':4783063,'Country Area km2 (mi2)':'270,467 (104,428)','Independence Day':'26 September 1907'}]\n",
    "\n",
    "# Create a Pandas DataFrame from a list of dictionaries\n",
    "countries = pd.DataFrame(list_of_countries)\n",
    "\n",
    "# Display the DataFrame\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb1fd3-d59e-4df7-b83c-9d084bb1b5ef",
   "metadata": {},
   "source": [
    "Looking at our DataFrame, we notice the information about the country New Zealand was repeated twice in rows 1 and 5. Also, we notice the Country Area has values in both square kilometers and square miles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd095a96-ebd8-4532-b8da-c7ec482d288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of the DataFrame columns\n",
    "countries.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b7844-c44e-49e7-b413-ce9c7d87a929",
   "metadata": {},
   "source": [
    "Examining the DataFrame using the info() attribute shows that both country area and independence day columns were assigned as text datatypes. \n",
    "\n",
    "In order to clean up the data for further analysis, we need to perform the following steps: \n",
    "\n",
    "* Split the Country Area values into two columns for both kilometer values and square miles\n",
    "* Remove any non-numeric characters from the area values\n",
    "* Change the country area and independence date columns to the correct data types\n",
    "* Drop unwanted rows and columns\n",
    "* Rename all the columns to have lower case letters separated by underscores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53d44f-9f0d-4259-afc4-b1ac9b9d1874",
   "metadata": {},
   "source": [
    "#### 6.1 Split Country Area\n",
    "\n",
    "The country area column is represented in both square kilometers and square miles. The square miles values are included within parentheses () and there is an empty space between the square kilometers and square miles values. Therefore, we can use the Pandas built-in split() function to separate these values into new different columns as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6b1f5-5313-4887-b0cf-f671be4776af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply split() function to separate values into new different columns\n",
    "countries[['Area km2','Area mi2']] = countries['Country Area km2 (mi2)'].str.split(' ', expand = True)\n",
    "\n",
    "# Display DataFrame head\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aadba42-efec-4da5-aeb1-bdd1d0c5e947",
   "metadata": {},
   "source": [
    "The first part of the code countries[['Area km2','Area mi2']] on the left side of the equal `=` sign creates two new DataFrame series. The split() function was used on the Country Area km2 (mi2) Series to separate the numerical values into square km and mile respectively. Note that we needed to tell the function to use the quotes â€˜ â€™ with space inside as the splitting point and to send each of the generated values into a new column using the expand parameter. \n",
    "\n",
    "The split() is one of the many built-in string formatting methods that can be applied to the Pandas series using the format Series.str.<function/property>. The following figure demonstrates the result of using the split() function in our DataFrame.\n",
    "\n",
    "<img src='Images/split_function.png' class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb2449-294c-44c7-9192-5063082be1b9",
   "metadata": {},
   "source": [
    "#### 6.2 Replace Strings\n",
    "\n",
    "After we split the country area into two separate columns for square kilometers and square miles, we need to continue our work to convert these values into numeric format by removing any non-numeric characters such as parentheses and commas from the newly created columns Area km2 and Area mi2. \n",
    "\n",
    "To do that, we can use the built-in replace() function to replace occurrences of specific patterns in a given series with some other string. The function will take the first parameter as the targeted string or regular expression pattern, and the second parameter as the replacement value. The following code demonstrates how we replace any non-numeric values using the regular expression (\\D+) within the Pandas replace() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba9592-66fe-4d29-add3-16dc0934b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply regular expression patternto replace any non-numeric values\n",
    "countries['Area km2'] = countries['Area km2'].str.replace('(\\D+)','')\n",
    "countries['Area mi2'] = countries['Area mi2'].str.replace('(\\D+)','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31673148-46b6-4964-a99f-4d974a859424",
   "metadata": {},
   "source": [
    "<img src='Images/replace string.png' class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf61618-50dd-40a0-8f93-498495487a1f",
   "metadata": {},
   "source": [
    "#### 6.3 Change Columns DataType\n",
    "\n",
    "Once we separated and cleaned up the country area into the proper format, we can move forward to assign the country area and independent day columns to the correct data types. To do that, we will make use of Pandas astype() function to pass a Python dictionary representing the name of each column and the corresponding data type as shown in this code below:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40325774-d0da-4062-9fea-ebcab6fc00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change specific columns data types\n",
    "countries = countries.astype({'Area km2': 'int64', \n",
    "                              'Area mi2':'int64', \n",
    "                              'Independence Day':'datetime64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eff283-adc0-4428-a03e-d44d090a4053",
   "metadata": {},
   "source": [
    "#### 6.4 Drop Rows and Columns\n",
    "\n",
    "Any data processing task would often include removing duplicate records or unwanted columns. In our countries DataFrame, we notice such cases as repeated rows for New Zealand. As we already split the country area into two new columns, there is no need to keep the old country area column too. \n",
    "\n",
    "To remove unnecessary rows, we can make use of the Pandas drop() function to remove rows or columns by specifying label names and corresponding axes. The axis parameter can take two values:\n",
    "\n",
    "* 0:  to indicate the action will be taken at the row-level or\n",
    "* 1: to indicate the action will be taken at the column-level\n",
    "\n",
    "The following code demonstrates how to use the drop() function to remove unwanted rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ffe4d-f9d5-40f5-84ec-338aed7ce9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the old country area column\n",
    "countries.drop('Country Area km2 (mi2)', axis = 1, inplace = True) \n",
    "\n",
    "# To remove duplicate row for New Zealand\n",
    "countries.drop(5, axis = 0, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43b817-0725-45a0-88c7-33c9e60b18ae",
   "metadata": {},
   "source": [
    "If you have several duplicate rows, we can also make use of the Pandas drop_duplicates() function to keep only the unique rows in the DataFrame. The following code would achieve the same result as the second line of code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826fe11-a24b-427e-8750-330598e02e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3728e72-65c8-49f5-8956-60e679a20659",
   "metadata": {},
   "source": [
    "#### 6.5 Rename Columns\n",
    "\n",
    "Another common data processing task is to standardize any DataFrame column names by using lower case letters separated by underscores. To achieve this, we can make use of the Pandas rename() function by passing a dictionary with current and new column names as shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279dc54-31df-4ef8-b07e-f2330fb6ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "countries.rename(columns={'Country Name': 'country_name', \n",
    "                          'ISO Code': 'country_code',\n",
    "                          'Country Population': 'country_population',\n",
    "                          'Country Date of Independence': 'independence_date',\n",
    "                          'Area km2': 'area_km2',\n",
    "                          'Area mi2': 'area_mi2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73730feb-4d3e-48d1-a810-6eb04a70599b",
   "metadata": {},
   "source": [
    "Then, we can run the info() method to print a concise summary of the DataFrame and examine all the changes applied to this tutorial. We can compare the changes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5645f-60f7-4083-a055-860c587f9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429492fd-e61a-4274-b89e-99d21acd433c",
   "metadata": {},
   "source": [
    "The output shows that we added two new columns for country areas and changed the data types to support the needed information in each column. There are only five countries instead of six so no more duplicate rows exist. The final DataFrame looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925a2c4-9394-46ec-8fb3-7d4202b7f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3617bee-8e04-4c76-a466-f191376b0602",
   "metadata": {},
   "source": [
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f3d6a-ae11-40f0-a6d6-ae8760465e92",
   "metadata": {},
   "source": [
    "### 7.0 Pandas Merging & Joining Data <a class=\"anchor\" id=\"section_7\"></a>\n",
    "\n",
    "Data professionals often need to combine different data sources for analysis projects. For example, if you are working on a data science project to analyze how sports games impact food and beverage sales, you may need to collect several datasets such as game timetables, sports teamsâ€™ performance, sports venues, and capacity, as well as sales figures for multiple vendors. \n",
    "\n",
    "If you are using the Pandas library for your project, it's likely that each dataset is stored in a separate DataFrame. Luckily, the library provides a set of tools that allow us to merge and join multiple DataFrames to create large datasets for analysis. \n",
    "\n",
    "In this section, we will learn the two most common ways to combine DataFrames in the Pandas library:\n",
    "\n",
    "* **pd.concat([DataFrame1, DataFrame2]): Simple combining two or more Pandas dataframes in a column-wise or row-wise approach.**\n",
    "\n",
    "* **pd.merge([DataFrame1, DataFrame2]): Complex column-wise combining of Pandas dataframes in a SQL-like way.**\n",
    "\n",
    "The concat() function is used to add together one or more DataFrames. To demonstrate how it works, we will use the function to combine multiple toy DataFrames about popular sports tournaments like FIFA Soccer World Cup and Rugby World Cup. Each dataset has different pieces of information such as winning team, host country, attendance size as shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4e47c-fbf3-4d8a-b87c-df94ea233791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIFA World Cup Winning Teams\n",
    "df_fifa_world_cup_winners = pd.DataFrame({'year': [2018,2014,2010,2006,2002,1998],\n",
    "        'winner': ['France','Germany','Spain','Italy','Brazil','France'],\n",
    "        'host_country': ['Russia','Brazil','South Africa',\n",
    "        'Germany','South Korea','Japan']})\n",
    "\n",
    "# Display DataFrame\n",
    "df_fifa_world_cup_winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e433f5-72a2-4883-963b-c98f76549edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rugby World Cup Winning Teams\n",
    "df_rugby_world_cup_winners = pd.DataFrame({'year': [1999,2003,2007,2011,2015,2019],\n",
    "                                           'winner': ['Australia','England','South Africa','New Zealand','New Zealand','South Africa'],\n",
    "                                           'host_country': ['Wales','Australia','France','New Zealand','England','Japan'],\n",
    "                                           'venue':['Millennium Stadium','Telstra Stadium','Stade de France','Eden Park','Twickenham','Nissan Stadium'],\n",
    "                                           'attendance':[72500,82957,80430,61079,80125,70103]})\n",
    "\n",
    "# Display DataFrame\n",
    "df_rugby_world_cup_winners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37945365-83d6-4373-a6c1-6f6e234a2ce0",
   "metadata": {},
   "source": [
    "We first noticed the DataFrames above have some common information such as the year of the event, the winning team name, and the host country. However, the Rugby World Cup dataset has two extra columns: venue and attendance.\n",
    "\n",
    "Let's try to create a large dataset with all winning FIFA and Rugby world cup teams. The code below demonstrates how to use all the common column names to stack the two DataFrames on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27296ea4-e62e-4768-bb2d-1b6576bf0d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the 2 DataFrames using the concat() method\n",
    "df_teams = pd.concat([df_fifa_world_cup_winners[['year', 'winner', 'host_country']],\n",
    "                      df_rugby_world_cup_winners[['year', 'winner', 'host_country']]])\n",
    "\n",
    "# Display the DataFrame\n",
    "df_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadd87c-7c80-40ec-a22e-8f5e5b1a2d5f",
   "metadata": {},
   "source": [
    "We created a new DataFrame object called df_teams with 12 records from the two parent datasets. However, the resulting DataFrame raises some issues. \n",
    "\n",
    "First, it becomes impossible to identify if a given team was part of the original Rugby or Soccer datasets. Second, the new DataFrame object inherits the original index values from the parent datasets. This behaviour can be controlled by adjusting the concat() function parameters. The keys parameter can be used to track the data source by adding extra index values to the new DataFrame as shown in the example below. This feature would allow us to query and access specific subsets of the DataFrame using the newly assigned index value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce484cf6-630e-4b06-afb9-85bbc73f03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data source index values to the new DataFrame\n",
    "df_teams = pd.concat([df_fifa_world_cup_winners[['year', 'winner', 'host_country']],\n",
    "                      df_rugby_world_cup_winners[['year', 'winner', 'host_country']]], keys = ['soccer', 'rugby'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49b7fa-a707-421c-8193-3c6a72b7b062",
   "metadata": {},
   "source": [
    "In another scenario, we may prefer the new DataFrame to have totally new index values. This option can be achieved by setting the ignore_index parameter to true as shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d921e0-dc8f-49cf-a728-d088f6a8094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore old index values in the new DataFrame\n",
    "df_teams = pd.concat([df_fifa_world_cup_winners[['year', 'winner', 'host_country']],\n",
    "                      df_rugby_world_cup_winners[['year', 'winner', 'host_country']]], ignore_index = True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a31c1-5877-464b-9306-fa582a71daff",
   "metadata": {},
   "source": [
    "The concat() function also allows us to combine multiple datasets even with little or no common values among them. The newly generated dataset will include all columns from the original DataFrame, with missing values replaced with null or NaN as shown in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9791f84-5fae-41ab-a59d-02267abda8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new DataFrame will include all original columns\n",
    "df_teams = pd.concat([df_fifa_world_cup_winners,\n",
    "                      df_rugby_world_cup_winners])\n",
    "\n",
    "# Display the DataFrame\n",
    "df_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595d81a-b7d3-46d2-bba4-7aea56f62011",
   "metadata": {},
   "source": [
    "The examples above demonstrate how the Pandas concat() function can create new datasets by adding DataFrame objects on top of each other (row axis). The function also provides the possibility to add the DataFrames sideways (column axis). This option is controlled using the axis parameter when it's set to 0 or 1 as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f611d-9fdc-44af-b520-2375b02089f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new DataFrame will include all original columns aligned horizontally\n",
    "df_teams = pd.concat([df_fifa_world_cup_winners,\n",
    "                      df_rugby_world_cup_winners], axis = 1)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a147a-b677-4705-b7ea-b383e69a8e01",
   "metadata": {},
   "source": [
    "Pandas merge() provides the functionality to join DataFrame and Series objects in a way similar to relational database operations. Users who are familiar with merging datasets using SQL but new to Pandas might be interested in this comparison. In this set of examples, we will demonstrate the use of the merge() function and highlight the use of some important parameters. \n",
    "\n",
    "The below code will create two DataFrame objects with one similar and four different columns, namely, key, column_A, column_B, column_C, and column_D. The key column includes some similar values that appear on both DataFrames as well as uncommon ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17d600-dd0b-4855-95ea-1c8675c660ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create left DataFrame\n",
    "left = pd.DataFrame(\n",
    "{\n",
    "    \"key\": [\"K0\", \"K1\", \"K2\", \"K3\", \"K4\", \"K5\"],\n",
    "    \"column_A\": [\"A0\", \"A1\", \"A2\", \"A3\", \"A4\", \"A5\"],\n",
    "    \"column_B\": [\"B0\", \"B1\", \"B2\", \"B3\", \"B4\", \"B5\"]})\n",
    "\n",
    "# Create right DataFrame\n",
    "right = pd.DataFrame(\n",
    "{\n",
    "    \"key\": [\"K0\", \"K1\", \"K2\", \"K3\",\"K6\"],\n",
    "    \"column_C\": [\"C0\", \"C1\", \"C2\", \"C3\",\"C6\"],\n",
    "    \"column_D\": [\"D0\", \"D1\", \"D2\", \"D3\",\"D6\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac8ddd7-e45c-4309-bc4d-d4e1a4355376",
   "metadata": {},
   "source": [
    "To join the two DataFrame objects using the  merge() function, we can use a set of parameters to identify common values, joining type, and source object. \n",
    "\n",
    "* on: This parameter can be used if the common column has the same name in both DataFrames\n",
    "* left on: Identify the joining column in the left DataFrame\n",
    "* right on: identify the joining column in the right DataFrame\n",
    "* indicator: Add an extra column to the joined DataFrame to show the source of each column\n",
    "* how: Identify the joining type as one of four possible options [inner, left, right, outer]. \n",
    "\n",
    "The following code will merge the two tables using the key column and the default inner joining method. We notice that only four records we selected represent the key values that appear in both DataFrames [k0, k1, k2, k3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef97f8e-4925-4584-a8ec-e063a85e92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames using the common column\n",
    "result = pd.merge(left, right, on=\"key\", how = 'inner', indicator = True)\n",
    "\n",
    "# Display the DataFrame\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019d0ef-8151-4e81-9b01-1ee314dded66",
   "metadata": {},
   "source": [
    "By changing the how parameter to outer value, we notice the joined DataFrame includes all records from both original DataFrames as shown in the example below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e0486-7949-4b51-b7aa-4f794d0163ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames using the common column\n",
    "result = pd.merge(left, right, on=\"key\", how = 'outer', indicator = True)\n",
    "\n",
    "# Display the DataFrame\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bab32-d3b9-4074-b6a5-13ece6713b72",
   "metadata": {},
   "source": [
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c507998-f3df-4a88-91fb-1c7033cff7da",
   "metadata": {},
   "source": [
    "### 8.0 Data Accessing & Aggregation <a class=\"anchor\" id=\"section_8\"></a>\n",
    "\n",
    "So far in this course, you have learned about Pandas different techniques to process and get data ready for analysis. In this section of the course, we will learn about how to explore your data by performing the following tasks:\n",
    "\n",
    "Selecting data by row, column and index values\n",
    "Filtering data with conditions\n",
    "Aggregating and sorting data\n",
    "\n",
    "To demonstrate these tasks, we will use the following toy DataFrame about different countries' information. The DataFrame has an index value representing each country's ISO code, regions, population size, and most common language in each country. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b97fa0-19bb-427d-b445-96c5c1b9fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries Information DataFrame\n",
    "df_countries_info = pd.DataFrame({'country_name': ['Egypt','Kenya','Morocco','Nigeria','South Africa','Brazil','Canada','Chile','Mexico','United States','China','India','Indonesia','Japan','Vietnam','Austria','Belgium','France','Italy','United Kingdom','Australia','Fiji','New Zealand','Tonga','Tuvalu'],\n",
    "                                  'region': ['Africa','Africa','Africa','Africa','Africa','South Americas','North Americas','South Americas','North Americas','North Americas','Asia','Asia','Asia','Asia','Asia','Europe','Europe','Europe','Europe','Europe','Oceania','Oceania','Oceania','Oceania','Oceania'],\n",
    "                                  'population':[100388073,52573973,36471769,200963599,58558270,211049527,37411047,18952038,127575529,329064917,1433783686,1366417754,270625568,126860301,96462106,8955102,11539328,65129728,60550075,67530172,25203198,889953,4783063,110940,11646],\n",
    "                                  'main_language':['Arabic','English','Arabic','English','English','Portuguese','English','Spanish','Spanish','English','Mandarin','Hindi','Indonesian','Japanese','Vietnamese','German','Dutch','French','Italian','English','English','English','English','English','English']}, \n",
    "                                 index = ['EG','KE','MA','NG','ZA','BR','CA','CL','MX','US','CN','IN','ID','JP','VN','AT','BE','FR','IT','GB','AU','FJ','NZ','TO','TV'])\n",
    "\n",
    "# Display DataFrame\n",
    "df_countries_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38685954-d974-4e5d-8899-88cbc8b5198e",
   "metadata": {},
   "source": [
    "Pandas library provides multiple ways to select a group of rows and columns by labels or position values using loc and iloc functions. loc is a label-based selection function where users must specify rows and columns based on the row and column labels; while iloc is an integer position-based selection function where users must specify rows and columns by the integer position values (0-based integer position). \n",
    "\n",
    "In the example below, we apply both loc and iloc to select a record based on its index label â€˜CNâ€™ or its 10th position in the DataFrame. We notice both methods would return a series object about China country information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f8e31-4d3d-4fdd-bb7f-aaa67f3bfd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one record\n",
    "df_countries_info.loc['CN']\n",
    "\n",
    "# Select one record\n",
    "df_countries_info.iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9232cb86-1294-4a30-a03a-338e7300df32",
   "metadata": {},
   "source": [
    "We can also pass a list of index labels or position values as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6824c8-65ca-4827-96ff-f775b16b7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a list of records\n",
    "df_countries_info.loc[['CN','NZ','GB']]\n",
    "\n",
    "# Select a list of records\n",
    "df_countries_info.iloc[[10, 19, 22]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79087f40-2331-4748-9238-8af86f4df302",
   "metadata": {},
   "source": [
    "We can also pass a range of index labels or position values using the colon sign : as shown in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59cd9e7-1518-43a2-b13c-3d7bbf894a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a range of values\n",
    "df_countries_info.loc['CN':'NZ']\n",
    "\n",
    "# Select a range of values\n",
    "df_countries_info.iloc[10:22]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224d705-09ec-4de2-a370-5dc1898e216d",
   "metadata": {},
   "source": [
    "We can also pass different DataFrame labels and positions as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698036be-aeaa-4966-9904-1aa25e5c53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a list of records and columns\n",
    "df_countries_info.loc[['CN','NZ','GB'],['region','population']]\n",
    "\n",
    "# Select a list of records and columns\n",
    "df_countries_info.iloc[[10, 19, 22],[1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ce351-8cdb-4e1b-a3bf-ff67cf25d5ed",
   "metadata": {},
   "source": [
    "We can also select rows by adding filter conditions that only match a subset of records. Each individual condition is often surrounded by parentheses () and several conditions can be grouped together using AND and OR conditions represented with & or | symbols respectively.   \n",
    "\n",
    "In the following example, we retrieve all records with the main language being English and the region being Oceania. Note that we left the column selection area empty to indicate that we want all DataFrame columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee3600-e688-44b0-8026-f01727d662e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select records based on list of condtions\n",
    "df_countries_info.loc[(df_countries_info['main_language']=='English') & \n",
    "                      (df_countries_info['region']=='Oceania'),]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283836a1-1c6c-4323-be56-3dacaa54938c",
   "metadata": {},
   "source": [
    "Sometimes we may need to sort a DataFrame or query output by specific numeric, alphabet, or date values. This process can be achieved by applying sort_values() function which takes the name of the targeted sorting value as a mandatory parameter and ascending order as default behaviour. We can adjust the above example by sorting the result by population size as shown in the code below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bbc81-3b3f-44df-b9bb-1cf559652e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame data\n",
    "df_countries_info.loc[(df_countries_info['main_language']=='English') & \n",
    "                      (df_countries_info['region']=='Oceania'),].sort_values(by='population')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ffe45-5bdb-4a5c-baf1-025e42345753",
   "metadata": {},
   "source": [
    "Finally, we will learn about how to query specific numerical values per group of records. In our toy example, we may want to know the summarization of population size per region or main language. This query can be answered by applying the Pandas groupby() function on the targeted groups and a summarization function on the numerical value. In the examples below, we calculate the total population size per language and geographic region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34328a-d766-41c6-841a-83a6ba2c366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total population size by main language\n",
    "df_countries_info.groupby('main_language').population.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb9389-3ee3-4055-994d-8b0d6f7106a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total population size by region\n",
    "df_countries_info.groupby('region').population.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d030139-1c9d-4675-a2b2-888ed05b730e",
   "metadata": {},
   "source": [
    "**[Back to Top](#title)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b12a4-34de-4a7f-b20f-d14849a64f54",
   "metadata": {},
   "source": [
    "### 9.0 Pandas Data Visualization <a class=\"anchor\" id=\"section_9\"></a>\n",
    "\n",
    "So far in this course, we have learned about many of the features that make Pandas one of the most popular data analysis and manipulation libraries. An essential part of any data analysis project is to perform a comprehensive exploratory analysis to help understand underlying patterns among the dataset variables.\n",
    "\n",
    "Although Pandas is not designed for comprehensive data visualization, it is capable of creating basic and practical plots. \n",
    "\n",
    "In this section, we will learn how to use Python Pandas to create two types of data visualizations:\n",
    "\n",
    "* **Chart Visualization**: Graphical representation of data using graphs such as histograms, pie charts, box plots, and so on. \n",
    "* **Table Visualization**: Graphical representation of data using HTML styling option within Pandas DataFrame objects. \n",
    "\n",
    "\n",
    "Pandas chart visualization can be achieved using the plot() method on Series and DataFrame objects. The function includes several parameters to identify the needed chart type as well as the targeted data columns. In this series of examples, we will use the plot() function to create some popular chart types. \n",
    "\n",
    "Time Series is one of the most commonly used charts to represent numerical values that change over a defined period of time. We can see this type of chart in visualizing financial market data like sales and price movement. In the Pandas library, the plot() method can be applied to a Series object with index values representing date values. Note that the line chart is the default type in the plot() method. Therefore, we do not need to change the type parameter. The following example demonstrates how a time series plot is used for visualization as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8da51-62c7-4d64-9d90-36c89c0dcf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset as a CSV file\n",
    "df_air_passengers = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv')\n",
    "\n",
    "# Set the Month values as the DataFrame index\n",
    "df_air_passengers.set_index('Month', inplace = True)\n",
    "\n",
    "# Display DataFrame\n",
    "df_air_passengers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a827286-fa9b-4e1f-a2b5-3bcc93bf5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DataFrame\n",
    "df_air_passengers.plot(figsize= (14,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08f47d-b917-4ada-9e90-42bf7b83f496",
   "metadata": {},
   "source": [
    "Another commonly used visualization type to examine the relationship between two different numerical variables is a scatter plot. In the example below, we use the popular iris dataset to explore the relationship between petal length and width variables. We can use color grouping to add additional analytics dimensions. In this example, we are adding color groups based on each flower species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93403d0b-3cae-4e13-a205-4160b54d6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset as a CSV file\n",
    "df_iris = pd.read_csv('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv')\n",
    "\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c229f-25e8-4d9b-b353-d9752e62461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a scatter plot to examine the relationship between variables\n",
    "df_iris.plot(kind = 'scatter', x = 'petal_length', y = 'petal_width', figsize= (14,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f31b2-6562-4e6f-9cb4-9abc4c3485b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map species to different color values\n",
    "col = df_iris['species'].map({'setosa':'b', \n",
    "                              'versicolor':'r', \n",
    "                              'virginica':'y'})\n",
    "\n",
    "# Apply a scatter plot to examine the relationship between variables\n",
    "df_iris.plot(kind = 'scatter', x = 'petal_length', y = 'petal_width', c=col, figsize= (14,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5bb89-bc90-478d-a03e-68e6fe5e9fb9",
   "metadata": {},
   "source": [
    "Pandas table visualization can be achieved by adding styling instructions to DataFrame objects in order to be rendered as CSS styles. In the example below, we apply the horizontal bar style to the variable sepal_width while the data is sorted in descending order. We notice in the top row, the bar line covers the entire cell and its size countune to get smaller for the rest of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef1ef6-0aec-49e6-8fbd-c1ad2d43137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bar style to sepal_width variable\n",
    "df_iris.sort_values(by='sepal_width', \n",
    "                      ascending=False).style.bar(subset=['sepal_width'], \n",
    "                                                 color='#00AEE8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658895a6-d5a2-4581-8276-2066d776658d",
   "metadata": {},
   "source": [
    "**[Back to Top](#title)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
